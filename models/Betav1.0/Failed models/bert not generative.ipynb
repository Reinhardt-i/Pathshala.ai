{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b3990cf-03c6-46b4-aca9-e016e5f635c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ceb5dc4cae45f38e9f04e25b4589c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  97%|#########6| 640M/660M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/sagorsarker/bangla-bert-base/187685debb02ef68d1f006babe89269074821c576de8b6f792f8811dd4c5ff6d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1732469488&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMjQ2OTQ4OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9zYWdvcnNhcmtlci9iYW5nbGEtYmVydC1iYXNlLzE4NzY4NWRlYmIwMmVmNjhkMWYwMDZiYWJlODkyNjkwNzQ4MjFjNTc2ZGU4YjZmNzkyZjg4MTFkZDRjNWZmNmQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=cMpYfpTxcUgODcp11Z94MxLOlhNw8ZtnRHK6NVej-eTixpEt7j%7EKBuiIMfK1brgfijvtKvCu370VhoqPoutdr8lRGcc9NIbt1YnTkYcR9wjomHNv3TmapINdRTJAw7rvfk89fbc56W9CqVmEQ5V01EckU76Ym3UusNw4rEQDhGA8JT0zZ35LOOA7FznHJQLfGNPPuBduFYG6lZYcMpLbj9GSwsHkYvUyScB6-USSuoaXDL%7Ef1hYFNuHrmaQsHRwl6GQUE%7EiOgnabhneXUhLRmQ777FDF-UVLJVHpmqrGtG0IdIy%7EsP9iH7xM1L0fk8fjAK1ukC3P7udGDYdif-tqQw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da1c1a5a8b04b5cbac49b464460fc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  97%|#########6| 640M/660M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at sagorsarker/bangla-bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at sagorsarker/bangla-bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "\n",
    "\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"sagorsarker/bangla-bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f542aaa-95ba-4ca9-825d-e4f4dc922210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(\"csebuetnlp/banglabert\")\n",
    "model_2 = AutoModelForPreTraining.from_pretrained(\"csebuetnlp/banglabert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1ab5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677cec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: à¦†à¦®à¦¿ à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦—à¦¾à¦¨ à¥¤\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "\n",
    "# Define a function for text generation with sampling\n",
    "def generate_text(seed_text, max_length=20, top_k=10):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(max_length):\n",
    "        # Add a mask token at the end of the seed text\n",
    "        input_text = generated_text + f\" {tokenizer.mask_token}\"\n",
    "        \n",
    "        # Tokenize input and ensure it's on the correct device\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Predict the masked token\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Find the index of the [MASK] token\n",
    "        mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "        # Get the logits for the masked token\n",
    "        mask_token_logits = logits[0, mask_token_index, :].squeeze()\n",
    "\n",
    "        # Apply top-k sampling\n",
    "        top_k_logits, top_k_indices = torch.topk(mask_token_logits, top_k)\n",
    "        probabilities = torch.softmax(top_k_logits, dim=-1)\n",
    "        \n",
    "        # Sample a token from the top-k logits\n",
    "        sampled_token_id = random.choices(top_k_indices.tolist(), weights=probabilities.tolist(), k=1)[0]\n",
    "\n",
    "        # Decode the predicted token and add it to the generated text\n",
    "        predicted_token = tokenizer.decode([sampled_token_id]).strip()\n",
    "        generated_text += \" \" + predicted_token\n",
    "\n",
    "        # Stop if the prediction is a sentence-ending token\n",
    "        if predicted_token in [\"à¥¤\", \".\", \"!\"]:\n",
    "            break\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "seed = \"à¦†à¦®à¦¿ à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦—à¦¾à¦¨\"\n",
    "generated = generate_text(seed, max_length=20, top_k=10)\n",
    "print(\"Generated Text:\", generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
